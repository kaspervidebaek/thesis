\title{Theory}

\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\author{Kasper Videbæk}
\date{\today}
 \usepackage{amsmath}
\usepackage{graphicx}
\usepackage[numbers]{natbib}
\usepackage{todonotes}
\usepackage[hidelinks]{hyperref}
\usepackage{verbatim}
\usepackage{multicol}
\usepackage{listings}
\usepackage{epstopdf}
\usepackage{wrapfig}
\usepackage[noend]{algpseudocode} 
\usepackage{algorithm}
\renewcommand{\algorithmicforall}{\textbf{for each}}

\lstdefinelanguage{CSharp}
{
sensitive=true,
morekeywords=[1]{
abstract, as, base, break, case,
catch, checked, class, const, continue,
default, delegate, do, else, enum,
event, explicit, extern, false,
finally, fixed, for, foreach, goto, if,
implicit, in, interface, internal, is,
lock, namespace, new, null, operator,
out, override, params, private,
protected, public, readonly, ref,
return, sealed, sizeof, stackalloc,
static, struct, switch, this, throw,
true, try, typeof, unchecked, unsafe,
using, virtual, volatile, while, bool,
byte, char, decimal, double, float,
int, lock, object, sbyte, short, string,
uint, ulong, ushort, void},
morecomment=[l]{//},
morecomment=[s]{/*}{*/},
morecomment=[l][keywordstyle4]{\#},
morestring=[b]",
morestring=[b]',
}

\begin{document}
\maketitle
\setcounter{tocdepth}{2}

\tableofcontents

\clearpage 
\section{Introduction}

\clearpage 
\section{Background}
In this section we give a background for the problem statement of this thesis. We briefly describe programming language concepts, version controls systems and the concept of automatic merging, which all contribute to understanding the problem we investigate.

\subsection{Version control systems}
The overall goal of version control systems is to support development of documents, by keeping track of changes over time. In the context of software development, version control systems are frequently used to keep track of source code. In the context of this thesis, we do not distinguish between these two - but abstract all the different merging-scenarios possible into the simple concept of merging two files, the \textit{branches}, with the \textit{base}; a third file that they both evolved from. Merging is the process of producing a single file that contains all the changes in the source that the two branches produced.

Merging source code is an inherent part of software development, and has gone through some development and automation since the first version control systems. Merging algorithms generally work by matching content between two branches and the common ancestor, and using the matchings it is possible to merge the content into one common file. If this is not possible a conflict is presented. Merging tools of version control systems are mostly line based; oblivious to the structure of the content of the files contained. The content they match is lines in the two branches and the common ancestor. 

A line based approach to works relatively well with files that are structured for computer input but also readable by humans, such as many source files and XML files. It works well, since line breaks often are used to make the structured data easily readable for humans; it is common for XML files to have opening and closing XML-tags on their own lines, and conventions of most programming languages have only one statement per line and opening block on lines of their own. In some programming languages line breaks are a part of the structure of the code - which makes them even more suitable for this approach.

Conventions of XML and source code can be broken. An entire XML-file can be a single line, and have the exact same meaning. Likewise with most source code - a function body can be written on a single line in many programming languages, without the compiler caring. A structured merging approach can clearly be helpful for files that are not structured with line breaks. However, this is not a very common case for files inside version control systems. The better case for improving version control by introducing structural knowledge, is where a single line contains structure.

\todo[inline]{Dette hører nok til et andet sted}
\todo[inline]{Er formålet med dette speciale at fixe "ikke bruger-læsbar-merging"?}
\todo[inline]{Tal om autogenereret kode, som placeres i VCS}

A conflict can be defined in quite a few ways for line based tools. In the Unix tool diff3, a conflict is a sequence of lines, that is changed in both branches, and is not separated by a single line that is unchanged in both branches. Such a conflict detection scheme is clearly an approximation. It is oblivious to the structure of the data and one could craft input that output invalid structural data. Further, the  actual meaning of the merged data is not considered at all - even if it is structurally valid, it might semantically be nonsense, even though a conflicts is not produced - see figure \ref{WrongMergeFromLineBasedApproach}.

\begin{figure}

\begin{multicols*}{4}

\texttt{A}


\begin{lstlisting}[language=CSharp]
f1();
if(e) {
f2();
f3();
}
\end{lstlisting}

\columnbreak

\texttt{O}

\begin{lstlisting}[language=CSharp]
f1();
f2();
f3();
\end{lstlisting}

\columnbreak

\texttt{B}
 
\begin{lstlisting}[language=CSharp]
if(e2) {
f1();
f2();
}
f3();
\end{lstlisting}

\columnbreak

\texttt{Result}
 
\begin{lstlisting}[language=CSharp]
if(e2) {
f1();
if(e) {
f2();
}
f3();
}

\end{lstlisting}
\end{multicols*}
\caption{A line based merge that produces no conflict, but produces a result, which is most likely not the intend of the users; $f3$ is not conditioned by $e2$, which no branches specified and $e$ only conditions $f2$ and not $f3$ as specified in the A-branch}
\label{WrongMergeFromLineBasedApproach}

\end{figure}


\subsection{Automatic merging}
The reason merging tools are line based these days, can probably be attributed to such an approach working relatively well with source code, XML and other files often found in repositories. Structure matching most definitely is a more time consuming task, and given the general use case of commits in version control, it might not be worth the extra computation time. Further, the semantics of programs are generally undecidable; in the most general sense we are unable to prove if a program works as intended. This is true for both the two branches, and the base of our merging, and since we cannot prove the behaviour of those we cannot say anything about if the merge works as intended either.

One very commonly used algorithm for merging is Diff3. \citet{Khanna} investigated it formally by regarding a file as a sequence where each line was an item in the sequence. They set up a number of expectations of what one expects from a merge, and showed that many of these assumptions do not hold: Conflicts can happen even when only unrelated regions are changed, differencing after a merge does not produce empty output, merging fails when both branches are very different from the ancestor and so forth. Even though this is the case, Diff3 continues to be widely used with few complaints of it shortcomings - it seems that the situations where Diff3 breaks are quite uncommon in version control systems.

Common algorithms for merging are state based. They compare the final state of branches, and creating a series of operations that transform one document from one state into another. \citet{Lippe} described a different approach, where editing tools remember the operations used to transform the data, and where the merge process is a matter of applying these transformations in the right order to minimize the number of conflicts.

%\todo[inline]{Mention whitespace ignoring}
%\todo[inline]{Mention horizontal differincing}
%\todo[inline]{Talk about prettyprinters.}
%\todo[inline]{Point out that merges might not compile.}


\subsection{Programming languages}
In this section we look into the process that a program goes through in the process up until execution, to give a simple overview of how this process gets us to syntax trees, and to look into what kind of consequences a this process has for regenerating concrete syntax when given a syntax tree.

\subsubsection{Grammars}
The process of turning programs into execution often relies on lexers and parsers. A lexer divides a programs into tokens. A token is any concrete string of text that is legal at any point in the program text. It can be numbers, strings, keywords, parenthesis, and any other character or sequence of characters that is legal at some point in the program text. 

After the lexer has divided the program into tokens, these are passed to the parser. The parser contains a set of rules of allowed token sequences - it defines whether or not a given token is valid. Given a grammar of a language, we can parse the syntax of a legal program, or generate syntax errors for invalid ones. When parsing it is possible to visit each part of the concrete syntax, making the construction of a syntax tree possible.

\subsubsection{Syntax Trees}
The purpose of parsing source code is often iterating over the elements of it to generate a syntax tree. A syntax tree is a tree representation of the program. The syntax tree representation is useful for further processing of the semantics of a program. At this point a lot of details about the concrete textual representation of the code has also left behind.

Depending on what kind of choice of syntax tree is done, information of the concrete syntax might be lost - several grammatical rules might produce the exact same syntax node.

In C, for example, the construct $a[i]$ is a shorthand for $*(a + i)$ and should produce the same semantic behaviour. Rewriting them both into the same structure in the syntax tree is one way to resolve this. Doing this also means that given a syntax tree it is harder, or impossible, to determine what the concrete syntax was.

Given the purpose of this thesis, the choice of syntax tree is quite important for us: We traverse syntax nodes and want to generate concrete syntax - which means we want a syntax tree that is as closely corresponding to the grammatical constructs of the language.

\subsubsection{Pre-processing and comments}
When looking at the syntax tree quite a lot of information is not immediately available: Everything from the preprocessing phase, syntactic sugar, whitespace and comments. While this information is not immediately available in the structure of the syntax tree most of this information can be found, by looking at the text that generated each node in the tree. 

\todo[inline]{Preprocessing}

\clearpage
\section{Related work}
In this section, we look into research that has already been done when embedding syntax tree knowledge into version control systems. Further we look into the general idea of differencing and merging general n-ary trees.

\subsection{Syntax Trees in Version Control}
There has been several attempts at creating more structure awareness in version control systems with several different goals. \citet{Freese} describes the idea of version control systems that understands code in a repository, and also have capabilities of migrating newly committed code automatically when a public API has changed.

Syntax tree differencing has generally been used to get overview of changes in version controls. \citet{Fluri} describes an approximating algorithm for trees, that  detects moves and and uses a heuristics approach to define how good a match a node is to another node - looking at both the value but also the child nodes. They also describe string similarity measures in which variable names with reordered words can be calculated as more similar and describe use cases for this. \citet{Hashimoto} also describes the idea of analysing code through syntax trees and discuss several applications. They presents an approach that works for four different object-oriented languages, and note that too fine-grained tree-edit-scripts are less comprehensible than more coarse-grained.

\citet{Hunt} describes the idea of using renaming and move-detection to avoid conflicts in language aware merging tools, and the idea of redefining the definition of conflicts to include some semantic conflicts, where the user should be warned about branches interfering in each others code. They do not provide much low level information about the algorithm.

\citet{Ekman} also looks into refactorings using the ideas of \citet{Lippe}. The idea is to log refactorings and normal code changes, they apply traditional changes first and thereafter apply refactorings. Further they discuss how preconditions can be detected and used to detect merge conflicts.

\citet{Apel} describes the idea of semistructured merge and creates a framework that is extensible to several languages. They parse programs into unordered trees in which the leaves are functions that contain unstructured program code. The code in leaves is then merged by language specific mergers or by an unstructured merge if no language language specific merge exists. They found that merge conflicts are generally minimized, and conclude that many conflicts are ordering conflicts. In 26\% of the cases there is more conflicts, due to renaming creating conflicts in this approach as opposed to compile errors in unstructured merge.

\citet{Olav} describes a system which first tries unstructured merge and afterwords does structured merge on conflicting JAVA files. The idea is to minimize the running time of the tools, to make them more convenient to use in real life scenarios. They report generally a lower number of conflicts than unstructured merge and \citet{Apel} but in some scenarios, in the case of moved files, they produce more conflicts. They general cut off 50\% of the running time compared to a pure structured approach, are a factor 12 slower than a unstructured merge, and a factor 6 faster than \citet{Apel}. However the improvement factor varies a lot from repository to repository.  

\subsection{Tree differencing and merging}
Research in the area has shown that the problem generally can be solved in cubic time. Several different algorithms exists, that all build on different dynamic programming decompositions. They perform well in different circumstances, and algorithms for cubic time have been created. \citet{Pawlik} describes and algorithm that performs in cubic time, and that dynamically decides if other algorithms perform better for a specific sub-problem - this algorithm seems to be the most promising in regards to differencing and running time.

\citet{Zhang} describes the idea of constraining mapping, such that the one of the common ancestors of two nodes needs to be the same. It provides an algorithm that runs in quadratic time. Later \citet{Lu} described a loosened constraint and provided an algorithm that was quadratic, but also dependant on the degree of the tree.

Tree-merging in itself is not a time consuming problem if an edit script is provided; the computational heavy part is to generate this edit script. The different existing merging algorithms that exists for three-way merging on trees generally describe different properties that are desirable for how to resolve conflicts, or when to produce them. \citet{lindholm} describes a merging algorithm for XML documents, \citet{Horwitz,Asklund,Olav} describes merging properties interesting for program code.

\todo[inline]{Ting der arbejder på tvaers af sprog: \\
- Semistroctored Merge: Rethinking Merge in Revision Control Systems \\
- Diff/TS: A tool for fine grained....
}
\clearpage

\section{Our algorithm}
Discussions of how the end algorithm should look.

\paragraph{Overall goal} The overall goal of the algorithm is to reduce the amount of \textit{trivial conflicts} found in version control systems. We define \textit{trivial conflicts} in section \ref{TrivialConflict}. We say \textit{trivial} conflicts since reducing conflicts in general cannot be a goal in itself: We cannot prove if a merge produces the correct behaviour of a program. Therefore we focus on solving only conflicts where a conflict either have no semantic meaning, or where we syntactically can see a meaningful way of reassembling a line that otherwise conflicts.

\todo[inline]{Ovenstående mål er vigtigt. Flet det ind i implementeringsafsnit}

Another part of the overall goal, is to keep the running time lower and more stable, than what we saw in tools and literature in the last section: Higher running times than $diff3$ are unavoidable if we want to parse the input files and match the syntax trees, however we can hopefully get a lower overall running time for the entire merging process, and hopefully have it become more stable for the general case, than what we have found.

\paragraph{Operations based vs. state based} While the idea of operation based merging on syntax trees is quite interesting, we choose a state based approach: Operation based merging require making an entire editor that can log operations, and the idea does not fit into VCSs. Further, it requires that every developer used this specific editor for their development, instead of their favourite tool. By making a state based approach all developers have to do is change their merging tool in their VCS.

\paragraph{Line-based vs. structure-based} We adapt the ideas of \citet{Olav} and \citet{Apel} for minimizing the running time of a state-based merging algorithm for code: Do structured merging when it's suitable.

\citet{Olav} uses structured merging on JAVA classes only when a line-based approach fails for the entire file. This lowers the average running time considerably for commits, but also creates a big variation from commit to commit: Whenever a file has a line-based conflict, the running time is impacted considerably due to the slowness of structured merging.

\citet{Apel} divides the process of merging code into two processes: One for matching functions, and one for merging them. The matching part is unordered and when functions have been matched, these can be merged as trees. They found that this minimizes conflict a lot, due to many conflicts being due to reordering. We use this division between matching and merging of functions too, however whenever we have matched a function, we do an unstructured merge on first, and only if this fails, we do a structured merge.

\subsection{Process for algorithm}
The overall merge algorithm can be seen in figure \ref{OverallMergingProcess}. We start with three input, files and try to do line based merging of them. If this succeeds, we return this result. If it fails, we parse the input files and serve them to a method which matches each function found in the parse with the same method in the other branches. These matches are passed to the line-based merger, which tries a textual merge of the function. If the line-based function merger fails, we pass the syntax tree of the function to the structure-merger which merges it structurally - with or without structural conflicts. When the structure merger and the line-based merger have parsed all functions, we produce a sensible ordering for the merged functions and reassemble, generate a complete result and return this.

\begin{figure}
   \centerline{\includegraphics[scale=0.55]{drawings/eps/overallmergingprocess.eps}}
   \caption{The overall process of merging a file.}
   \label{OverallMergingProcess}
\end{figure}

\paragraph{Running time} The running time of the algorithm is the sum of the following parts:

\begin{itemize}
\item The time for line merging the file.
\item If it conflicts:
\begin{itemize}
\item The time for parsing the file.
\item The time for matching functions in a conflicting file.
\item The time for line merging each function.
\item For conflicting functions:
\begin{itemize}
\item The time for structurally merging items.
\end{itemize}
\item The time for reassembling in a good order.
\end{itemize}
\end{itemize}

From a worst-case running-time perspective, this does not look good: In the worst case, we merge the entire file and hit a conflict on the last line. Then we start over, parse the files, match the functions, and try a line merge on each of these matches. Each function fail on the last line and we have to do a structure match for all of them. However, the approach is based on a couple of assumptions:

\begin{enumerate}
	\item Tree merging is much more time consuming than line-based merging.
	\item Only a small amount of files produce a line-based conflict.
	\item When a matching is found, only a few functions produce a line-based conflict.
\end{enumerate}

If these assumptions hold, the overall running time is lowered considerably compared to other structured approaches, since much more of our merging happens as line-merging. The first item on the list is true; both the size of the input problem and the asymptotic running time of the algorithm is larger as shown later. The second two are not as clear cut to provide backing for, but best-practice guides for version control systems generally recommend that commits are small logically connected parts. Given well-encapsulated code, this means that only a few functions are changed per commit.

\subsection{Trivial conflicts}
\label{TrivialConflict}
The goal described earlier was to minimize the amount of trivial conflicts found in the syntax tree. But what is a trivial conflict? A trivial conflict is a conflict that cannot be solved in line based tools, but where knowledge of the structure of the language allow us to make more informed choices about what is intended by the user.

\todo[inline]{Insert code examples in all of the below.}

\paragraph{Reordering of class-members} Class-members needs to have a definition order in concrete syntax, yet this order has no meaning for the execution in most languages. If one branch moves a method of a class, and another branch makes change to this branch, a line based tool report a conflict. An intelligent merge of this is to propagate the changes in one branches into the moved methods body.

Given a syntax tree, we know where method-definitions exists, and we can treat them as a set. This gives us a problem of matching functions together across the three branches; we need to create some heuristic that allow us to decide whether two functions are the same or not. After that can merge the function bodies of the methods matched from the three branches.

\paragraph{Reorderings in parameter-lists and argument-lists} Parameter-lists and argument-lists are closely tied together. A reordering of the parameters in a method definition in one branch cause all argument-lists to also be reordered. If the second branch adds a parameter at the same time, this also adds an argument in all calls to this function. From a line based perspective this generates conflicts. Given a syntax tree this can be solved by matching parameters and arguments across branches and create new argument- and parameter-lists, that both contain the reorderings and the added items.

\paragraph{Merging conflicting lines} A conflict in line based tools is when two branches change the same part of the code, where there is no unchanged line inbetween those changed parts. This seems like a good heuristic for when the code for two branches might interfere, but another reason to treat this as a conflict in a line based tool, is because the tool has no idea how to merge those lines. This is however not the case for a merging algorithm based on syntax trees; we can look at sub-nodes of the trees from different branches, and merge these.

\begin{figure}

\begin{multicols*}{3}

\texttt{A}

\begin{algorithmic}
	\State $a \gets 0+2+3$
\end{algorithmic}

\columnbreak

\texttt{O}

\begin{algorithmic}
	\State $a \gets 1+2+3$
\end{algorithmic}

\columnbreak

\texttt{B}
 
\begin{algorithmic}
	\State $a \gets 1+2+4$
\end{algorithmic}

\end{multicols*}
\caption{Three statements, that conflict in a line-based merge}
\label{ConflictingLines}

\end{figure}

Given three input lines, that are vaguely similar as in figure \ref{ConflictingLines} where both branches have changed small parts of the behaviour of the line, what should we merge it into? Both branches want to change small parts of behaviour, and as such producing a conflict could easily be seen as the best choice. However a simple re-factoring of the code like in figure \ref{RefactoredForNoConflict} makes a line based merge it without a conflict.

This show the arbitrariness of the line based merge approach; in this case it wont mean much due to it being a small integer expression, but it is easy to imagine more complicated expressions where the same refactoring makes a big difference. 

\todo[inline]{Adding a single, empty, line will be "remove" conflicts}

\begin{figure}
\begin{multicols*}{3}

\texttt{A}

\begin{algorithmic}
	\State $x \gets 0$
	\State $y \gets 2$
	\State $z \gets 3$
	\State $a \gets x+y+z$
\end{algorithmic}

\columnbreak

\texttt{O}

\begin{algorithmic}
	\State $x \gets 1$
	\State $y \gets 2$
	\State $z \gets 3$
	\State $a \gets x+y+z$
\end{algorithmic}

\columnbreak

\texttt{B}
 
\begin{algorithmic}
	\State $x \gets 1$
	\State $y \gets 2$
	\State $z \gets 4$
	\State $a \gets x+y+z$
\end{algorithmic}

\end{multicols*}
\caption{The lines from Figure \ref{ConflictingLines} refactored for merging without conflicts.}
\label{RefactoredForNoConflict}
\end{figure}

So what do we do? If we can match nodes in the syntax tree of the branches when they are similar, then we can do a top-down merge of nodes; only if the structure or content of two sub-nodes has changed different structure or content, we generate a conflict. This allows us to actively merge the branches in figure \ref{ConflictingLines} into $a \gets 1+2+4$ without user interaction.

\paragraph{Renaming identifiers} Local variables and class members have an identifier that is used to reference them from other places in the code. This identifier can be renamed. Renaming an identifier also has the consequence of changing the identifier all the places where this specific identifier is referenced. If a rename happens in one branch, and somebody changes or adds a line that references this identifier in another branch, the merge either give a conflict or a merges perfectly but leaves a compile error: 

\begin{itemize}
\item If somebody changes an expression around an identifier that has been renamed in another branch this generates a conflict in a line based approach. This might not produce a conflict in our approach, since we only modify parts of the sub-tree, but it might still do it.
\item If a identifier has been renamed in one branched, while somebody has added a new reference to it, in the opposite branch, this generate merge that gives an compile error.
\end{itemize}

It might be possible to detect rename of identifiers and apply this rename on the trees before doing an actual merge, and thereby avoid conflicts.

\subsection{Handling syntax trees}
\paragraph{Tree matching algorithms} Using regular tree matching algorithms on entire syntax trees takes cubic time of the number of nodes in the trees. Further, the number of nodes generated from a line of code can be quite high. Some initial tests on trees generated from C\# files, we encountered matching times of up to two minutes, from a relatively complex, but not unheard of 700-line C\# file that gave a syntax tree with around 3800 nodes. This problem however, becomes a lot smaller if we only use it on method bodies - then it might be a viable option.

Tree matching algorithms returns a matching as a flat list of node pairs. This list makes it easy, for each node, to find the corresponding node in the other tree, and makes it possible to calculate costs easily. This can also easily be stitched together to create a three-way matching such that all nodes in the O-branch is matched with everything in the B- and A-branches.

Our goal is to construct a tree that is the merge of the branches, and here it seems there are not really a natural starting point. First of all, the merging list does not have any idea of the structure of the tree we needs to construct, so the starting point needs to be an iteration over the three input-threes instead. In this iteration, however, the matching can be quite helpful: Whenever we encounter a change in either of the trees, we can look up the matching, and this gives us a hint about what has happened here. Resolving operations as deletions, insertions and updates are quite simple this way.

However, operations on trees can also be moves up or down in either of the branch trees, which makes this more complex: The matching does not tell us what happened, just that a match exists to somewhere in the other trees - this means that such an algorithm needs to search amongst parents and children to figure this out, and when this is done produce some the correct output. This might not be impossible, but the entire process becomes a quite complex at this point. 

We abandoned the general tree matching approach, and created a matching algorithm that worked entirely on the same levels of the two input trees: Move operations are not detected up and down in the tree. This removes the above challenge. After creating such a matching, we can do a top-down recursion and create a \textit{matching tree}; a tree where each leaf is a matching between the three trees. 

The next step is to chunk these trees. Chunking is the part of the matching algorithms that divide them into stable and unstable parts, to identify updates. This definition becomes quite mindbogling in the matching tree generated, and we wound up abandoning the general tree-matching approach altogether.
 
\paragraph{Semantic vs. non-semantic tree-merging} A syntax-tree has a built-in semantic, that cannot be broken. Each node has some specific children with a specific type and purpose. A syntax tree can also be viewed as a tree with generic nodes with arbitrary arity. The later option is the option that allow us to run general tree-merging algorithms on the syntax trees.

Abandoning general tree merging algorithms does not mean that we have to abandon the general tree approach to merging. The advantage of using a general tree is that we can manipulate the tree more freely; for example we can insert nodes to indicate conflicts or we can iteratively create a tree instead of creating a tree entirely top-down. Further, we can create generic algorithms that work on the semantic-free trees, which then later make it easier to make the merging algorithm cross-language.

The disadvantage is that we have to create an entirely new step that takes a semantic-free-tree and turns it into a semantic tree. This step can get quite complicated; what should happen if a subnode has been added as a child to a node, where we it does not semantically fit? This creates quite a few problems.

In the end, we decided to only work on semantic trees and we decided that our algorithm takes in the nodes from each branch, and return a string that is the concrete syntax that the merge of these nodes creates.

\clearpage
\section{Implementation}
In this section we dive into algorithms written for the code-merging tool. The first subsection lists a couple of utility functions that are needed for the later sections. The second describe the general file merging functions. In the third we describe the syntactic breakdown of files. In the fourth we describe our efforts to create merging of how classes, namespaces and other structures of source code are generated, and the last section focus on merging syntax trees created inside method-definitions.

\subsection{Utility methods}
In this section we describe series of utility methods that are used in the later sections for implementing a merging of the merging algorithms. By themselves they might seem meaningless. In some cases we motivate their existence in this section, but in other cases the motivation might first become clear in the later sections.

\subsubsection{Two-way sequence matching}
\begin{wrapfigure}{r}{0.4\textwidth}
   \centerline{\includegraphics[scale=0.4]{drawings/eps/mincostsequencematchingambigious.eps}}
   \caption{Two input sequence that has two results from the matching algorithm.}
   \label{TwoWayMatchingAmbigiouty}
\end{wrapfigure}

We often want to match equality between two sequences. We do this by solving the sequence alignment problem with the Needlemann-Wunch-algorithm. It allows us to minimize the penalty of alignment between two sequences, given a cost-function and a penalty for gaps in the output. In our case, we do not care about gaps in the output sequence nor do we have very specific costs: All that matters is if two items are equal, or not equal. The two-way sequence matching takes three parameters: Two sequences and a equality function. It runs Needlemann-Wunch with a gap-penalty at zero and the cost of equality set to -1 and while the cost of inequality is 1.

\paragraph{Result} Some inputs have several possibly minimum cost matchings. An example of an input that generate two different matchings can be seen in figure \ref{TwoWayMatchingAmbigiouty}.

\paragraph{Running time} The running time of Needlemann-Wunch is often stated to be $O(n \cdot m)$, where $n$ and $m$ is the number of items in the two input sequences. This has the implicit assumption that the function that compares two items runs on $O(1)$ time. Since this function is the equality function input to our two-way sequence matching function, we do not make such an assumption; the running time is $O(n \cdot m \cdot t)$ where $t$ is the running time for the equality function.

\subsubsection{Three-way sequence matching}
\begin{figure}
   \centerline{\includegraphics[scale=0.6]{drawings/eps/threewaymatching.eps}}
   \caption{A run of the Three-way matching-algorithm}
   \label{ThreewayMatching}
\end{figure}

Given three input sequences A, O and B we want to output a single sequence of matchings between them. A match is a data-structure, that contains three elements A, O and B, illustrated as a row in the result of Figure \ref{ThreewayMatching}. An empty result in any of the three columns on each row means that there is no matching from this specific input sequence.

As can be seen in Algorithm \ref{ThreeWayMatchingAlgorithm}, we two-way sequence-matching two times on the sequence-pairs $(A, O)$ and $(O, B)$, this gives two sequences of matchings.  We iterate over these two in parallel; starting at the first item of both and continue to the next item in specific circumstances:

\begin{itemize}
	\item If either of the items are at the end of the sequence. Add the opposite item to the result and iterate further in that matching sequence.
	\item If either of the items has an $o$-item that is $null$, add it this item to the result and iterate further in that matching sequence.
	\item Otherwise the O-item must be the same of both lists, and we can add both items to the result and iterate to next item in both matching sequences.
\end{itemize}

Each column in a matching keeps the order of the input sequences, and the intuition behind the $while$-loop in the algorithm is that the $O$-items in the two matching always show up in order.

\paragraph{Running time} The running time for the two-way alignment functions are $O(|A||O|t)$ and $O(|B||O|t)$, where $t$ is running time for $equality$. The result from these functions are maximally of the size $|A|+|O|$ and $|B|+|O|$ in the extreme where there exists no matching. The $While$-loop iterate at most $|A|+|O|+|B|$ times, and all operations inside it are constant time. This gives an overall worst-case running-time of:

\begin{equation}
	O(|A||O| t + |B||O| t + |A|+|O|+|B| ) \nonumber
\end{equation}  

\begin{algorithm}
\begin{algorithmic}
\Function{ThreeWayMatching}{$A$, $O$, $B$, $equality$}
	\State $ma\gets$ TwoWayAllign($A, O, equality$)
	\State $mb\gets$ TwoWayAllign($B, O, equality$)
	
	\State $a \gets 0$
	\State $b \gets 0$
	\State $rv \gets []$
	
	\While{$a+b < |ma| + |mb|$}
		\If{$a \geq |ma|$ or $b \geq |mb|$}
			\State add item from opposite matching to $rv$
			\State increase opposite counter
		\ElsIf{$ma[a].o$ is $null$ or $mb[b].o$ is $null$}
			\State add the matching whose $o$-item is $null$ to $rv$
			\State increase that matchings counter
		\Else
			\State add $ma[a]$ and $mb[b]$ to $rv$
			\State increase $a$ and $b$
		\EndIf
	\EndWhile
	\State \Return $rv$
	
\EndFunction
\end{algorithmic}
\caption{Three-way matching algorithm}
  \label{ThreeWayMatchingAlgorithm}
\end{algorithm}

\subsubsection{Chunking}
Given a three-way sequence matching, a single match can be defined as stable or unstable depending on whether or not all items $A$, $O$ and $B$ are present. This notion is quite useful for doing an actual merge. It is reasonable the assume that any stable match, does not not need to be merged.

\begin{wrapfigure}{r}{0.5\textwidth}
   \centerline{\includegraphics[scale=0.4]{drawings/eps/threewaymatching-chunking.eps}}
   \caption{Chunking a matching. Grey areas are unstable chunks and white areas are stable.}
   \label{Chunking}
\end{wrapfigure}

Another view could be that an unstable match should be viewed as either a deletion or an insertion. This however leaves us in a situation where all matchings can easily be resolved into a merge; something is inherently wrong with that approach. The problem lies in the fact that the matching sequence cannot represent the actual sentiment of the changes it is seeing: A deletion followed by an insertion might very well be an update of a single line. If both branches have done an update of the same line, then a conflict should be produced. Because of this we need to introduce chunking.

A chunk is three lists, one for A, one for O and one for B. A chunk can be stable or unstable. Given a sequence matching, we generate chunks by iterating over every single match. If the first element is stable, then we generate a stable chunk, and add it's three items to the appropriate list. We continue adding all elements until we hit an unstable match. When we hit an unstable match we generate an unstable chunk, and add all non-empty elements from the match to the appropriate list.

The pseudocode for this algorithm is in Algorithm \ref{CunkingAlgorithm}. \footnote{The ChunkBy method used in the algorithm can be found at \url{http://msdn.microsoft.com/en-us/library/cc138361.aspx}}

\paragraph{Result} Unstable chunks are not necessarily conflicts. In the result of Figure \ref{Chunking} the top item is an insertion of the $0$-item. The second chunk is an update where $3$ is turned into $4$ and the last chunk is not resolvable. 

\paragraph{Running time} The ChunkBy-method only performs constant time operations on each element in the input sequence, and we can conclude that it is running in $O(n)$ time, where $n$ is the number of items in the input matching. The number of chunks is unknown without looking at the input, however the total number of matches within all the chunks is $n$. Therefore the overall running time is $O(n)$ for the entire chunking-process.

\begin{algorithm}
\begin{algorithmic}
\Function{Chunking}{$matching$}
	\State $cs \gets matching.ChunkBy($no items in a matching are null$)$
	\State $chunks \gets []$
	\ForAll{$c$ in $cs$}
		\State $chunk \gets init new chunk$
		\ForAll{$m$ in $c.matches$}
			\If{$m.A \neq NULL$}
				\State $chunk.A.Add(m.A)$
			\EndIf
			\If{$m.O \neq NULL$}
				\State $chunk.O.Add(m.O)$
			\EndIf
			\If{$m.B \neq NULL$}
				\State $chunk.B.Add(m.B)$
			\EndIf
		\EndFor
	\EndFor
	\State \Return chunks
\EndFunction
\end{algorithmic}
\caption{Chunking algorithm}
  \label{CunkingAlgorithm}
\end{algorithm}


\subsubsection{Priority-chunking}
\label{PriorityDiff}
Our three-way matching function operates on equality. Yet in some cases we want to use heuristics functions that looks for similar items instead of absolute equality. To use such a heuristic function in the matching functions earlier defined, it has to answer either "similiar" or "not similar", and cannot distinguish between equality and similarity. However, sometimes we want to make sure that completely equal items are matched before similarity is used.

The solution to this problem is priority-chunking as shown in Algorithm \ref{PriorityChunk}. It starts by doing a matching on the three input sequences using a strict equality function. It then iterates through all chunks and for unstable chunks it does another matching using the similarity-function, and generates three types of chunks:

\begin{itemize}
   \item Equality-stable items
   \item Similarity-stable items
   \item Unstable items
\end{itemize}

\todo[inline]{Der mangler et eller andet i det her afsnit.}

\paragraph{Running time} The running time of the first ThreeWayMatching function is  $O(|A||O|t + |B||O|t)$ and it returns maximally $A+O+B$ items, which are then passed to the chunking function, which run in $O(n)$ time. This gives a worst case running time like below, for the first two statements in the algorithm.

\begin{equation}
O(|A||O|t + |B||O|t + |A|+|O|+|B|) \nonumber
\end{equation}

The for-loop around $chunks$ runs $|A|+|O|+|B|$ times in worst case. In each iteration the worst case running time is the same as above. This gives this overall worst case running time for the loop:

\begin{equation}
O((|A| + |O| + |B|) (|A||O|t' + |B||O|t' + |A|+|O|+|B|)) \nonumber
\end{equation}

This is, however, quite an exaggeration. The situation that this describes if every single chunk contains the entire input matching, which is obviously not true. It can be seen that:

\begin{enumerate}
\item If there are $|A|+|O|+|B|$ chunks, then the running time of ThreeWayMatching is $O(1)$
\item If there is 1 chunk, then the running time of the inner ThreeWayMatching is the same as for the outer ThreeWayMatching.
\end{enumerate}

A more accurate way of describing the running time of the loop of algorithm is the sum over each chunk, and their individual sequences from each input branch $c_a$, $c_o$ and $c_b$:

\begin{equation}
\sum_{c \in C} O(|c_a||c_o|t' + |c_b||c_o|t' + |c_a|+|c_o|+|c_b|) \nonumber
\end{equation}

We know that each chunk contains only items from the input sequences, and we know there is no overlap between chunks. Therefore, we know that:
\begin{equation}
\sum_{c \in C} |c_a| = |A| \nonumber
, \sum_{c \in C} |c_o| = |O| \nonumber
, \sum_{c \in C} |c_b| = |B| \nonumber
\end{equation}
Which means that we can simplify the above sum by moving out the sums:
\begin{equation}
O(|A| + |O| + |B| + \sum_{c \in C} |c_a||c_o|t' + |c_b||c_o|t)' \nonumber
\end{equation}

It we also know that

\begin{equation}
\sum_{c \in C} |c_a||c_o| \leq |A||O| , 
\sum_{c \in C} |c_b||c_o| \leq |B||O| \nonumber
\end{equation}

And thereby we can upper bound the above equation as:

\begin{equation}
O(|A| + |O| + |B| + (|A| |O| + |O| |B|) t') \nonumber
\end{equation}

Which leaves us with the overall running time of:

\begin{equation}
O((|A| + |B|)|O|t + |A|+|O|+|B| + (|A| |O| + |O| |B|) t')) \nonumber
\end{equation}

Where $t$ is the running time of $equal$ and $t'$ is the running time of $similar$. This can be simplified into:

\begin{equation}
O(|O| (|A| + |B|)(t + t')) \nonumber
\end{equation}

\begin{algorithm}
\begin{algorithmic}
\Function{PriorityChunk}{$A$, $O$, $B$, $equal$, $similar$}
	\State $matches \gets ThreeWayMatching(A, O, B, equal)$
	\State $chunks \gets Chunking(matches)$
	\State $outChunks \gets []$
	\ForAll{$chunk$ in $chunks$}
		\If{$chunk$ is stable}
			\State add $chunk$ to $outChunks$ as equality-stable
		\Else
			\State $innermatches \gets ThreeWayMatching $
			\State ~~~~~~~~~~~~~~~~~~~~~~~~~~~~ $ (chunk.A, chunk.O, chunk.B, similar)$
            \State $innerchunks \gets Chunking(innermatches);$
			\ForAll{$innerchunk$ in $innerchunks$}
				\If{$innerchunk$ is stable}
					\State add $innerchunk$ to $outChunks$ as similar-stable
				\Else
					\State add $innerchunk$ to $outChunks$ as unstable
				\EndIf
			\EndFor
		\EndIf
	\EndFor


	\State \Return outChunks
\EndFunction
\end{algorithmic}
	\caption{Priority-chunking algorithm}
	\label{PriorityChunk}
\end{algorithm}


\subsubsection{Two-way set matching with costs}
Finding a minimum cost matching of the bipartite sets \texttt{x} and \texttt{y} can be solved in cubic time using a maximum flow algorithm as described by \citet{bipartitecost}. A prerequisite of this algorithm is however that the two sets are equal in size. Further, it assumes that the input can generate a perfect matching in the internal graph. We want to modify the bipartite graph algorithm such that it can consume differently sized sets, and generate the best minimum cost matching, even though there is no perfect matching.

The \citet{bipartitecost} algorithm starts by generating a weighted undirected graph, $G$. In this graph each item of the input sets is a node in the graph, and for each pair of nodes an edge is generated with the cost between them as the weight. The algorithm defines a set of prices $p$ for for each nodes. For $x$-nodes the initial prices are initially 0 and for $y$-nodes the initial prices is equal to the minimum weight of the incoming edges. It then creates an residual graph $G_M$ given a matching. The residual graph contains all the nodes of the original graph $G$, and further a source node $s$, and a sink node$t$. The source node has zero-cost edges to all $x$-nodes not in the matching and each $y$-node has a zero-cost edge towards the sink not in the matching. For each edge in the original graph, we add an edge in the residual graph - if this specific edge exists in the matching we add it as an edge from $y$ to $x$, otherwise from $x$ to $y$. The price weight of these edges is the $price(xnode)+originalgraphweight-price(ynode)$. For each iteration of the algorithm it updates the prices such that the price in the next iteration is the current price and the cost to get from source to this node in the current residual graph.

The pseudocode for the algorithm can be found in Algorithm \ref{OriginalCostMatching}.


\begin{algorithm}
\begin{algorithmic}
\Function{BipartiteSetMatching}{$X$, $Y$}
	\State Generate $G$ from $X$ and $Y$ given a cost function
	\State Start with $M$ equal to the empty set
	\State Define $p(x)$ for $x \in X$, and  $p(y) = \underset{e \; into \; y}{\operatorname{min}} c_e$ for $y \in Y$
	\While{$M$ is not a perfect matching}
    	\State Find a minimum-cost $s-t$ path $P$ in $G_M$ with prices $p$
    	\State Augment along $P$ to produce a new matching $M'$
    	\State Find a set of compatible prices with respect to $M'$
    \EndWhile
	\State \Return $M$
\EndFunction
\end{algorithmic}
	\caption{Bipartite set matching algorithm}
	\label{OriginalCostMatching}
\end{algorithm}

We want to modify this algorithm to have the following properties:

\begin{itemize}
\item It works for x and y sets of different size. 
\item It works even though a perfect matching cannot be found in the residual graph.
\item For any element not matched it should return a matching $(x, \epsilon)$ or $(\epsilon, y)$
\end{itemize}

If the sets are not equal in size, we add nodes, auxiliary nodes, in the smallest of the two sets until the two sets are equal in size.\footnote{This concept is derived from a project I did in autumn 2012 with Torben Andersen} For each auxiliary node we add an edge to each node in the opposing set with a cost larger than the largest cost between any other pair of nodes. In the final matching, the matchings that contain auxiliary nodes should be considered as not matched; since they have been matched with the highest possible weight for their edges - the rest of the matchings are definitely the minimum cost matching.
\todo[inline]{Forklar det her bedre}

The second challenge is due to the fact that the end-condition in the loop requires a perfect matching. However if any node in any of the two sets does not have a cost defined for a single node in the opposite set, then a perfect matching is not possible. An example of such a situation could be $X={1, 2}$ and $Y={1, 3}$ with a cost function only defined for elements that are numerically equal. This is demonstrated below. \\

\begingroup
    \fontsize{7pt}{10pt}\selectfont
\begin{tabular}{ p{5.5cm} | p{5.5cm} }
   \centerline{\includegraphics[scale=0.3]{drawings/eps/TwoWayCostMatchingNotPerfect/1it0.eps}} &
    \centerline{\includegraphics[scale=0.3]{drawings/eps/TwoWayCostMatchingNotPerfect/1it1.eps}} \\
   Initial graph for the sets and cost function. &
    The first residual graph. \\ \\ \hline \\

    \centerline{\includegraphics[scale=0.3]{drawings/eps/TwoWayCostMatchingNotPerfect/1it2.eps}} &
    \centerline{\includegraphics[scale=0.3]{drawings/eps/TwoWayCostMatchingNotPerfect/1it3.eps}} \\
   Running dijkstra finds a matching. &
   In the second residual graph, all possible matchings have been found, but the matching is still not a perfect matching. \\ 
\end{tabular}

\endgroup

When our algorithm reaches the second iteration, the matching is still not perfect for the entire graph. The problem is that one or more edges have no edges in the initial graph. The solution seems to be to not generate node with no costs generated for them, and return them as matchings with an empty opposite. This is obviously a correct assumption; since they have no costs associated to the other set, they are of course impossible to match.

Looking at the same example, now that the node $(X: 1)$ has a cost to the node $(Y: 2)$. For this graph a perfect matching is not possible either. This situation is overcome by the generation auxiliary nodes after nodes with no cost have been deleted. As seen in the right figure below. \\

\begingroup
    \fontsize{7pt}{10pt}\selectfont
\begin{tabular}{ p{5.5cm} | p{5.5cm} }
   \centerline{\includegraphics[scale=0.3]{drawings/eps/TwoWayCostMatchingNotPerfect/NoFakeNode.eps}} &
    \centerline{\includegraphics[scale=0.3]{drawings/eps/TwoWayCostMatchingNotPerfect/FakeNode.eps}} \\
   A graph where all nodes have a cost, but where no perfect match is possible. &
    The same graph, with auxiliary nodes added to make a perfect match possible. \\ 
\end{tabular}

\endgroup

We now have three sets, that together be the final matching:
\begin{itemize}
   \item The matchings that have an original input item in both ends.
   \item The matchings that have an auxiliary node in either end.
   \item The set of $X$- and $Y$-nodes that were never added to the graph.
\end{itemize}

We union these sets and create matchings with an empty end apprioriately for the last two items in the list. The algorithm can be seen in Figure \ref{TwoWayCostMatching}.

\paragraph{Running time} The running time for the original algorithm is expressed in terms of the input size of both sets $n$ as $O(n^2 \log n)$. We have artificially expanded the smaller of the two sets. Therefore $n$ is $max(|X|, |Y|)$ in our case, and the running time is:

\begin{equation}
O(\max(|X|, |Y|)^2 \log (\max(|X|, |Y|)) \nonumber
\end{equation}


\begin{algorithm}
\begin{algorithmic}
\Function{CostSetMatching}{$X$, $Y$}
	\State Generate $G$ from $X$ and $Y$ given a cost function.
	\State ~~~~ Store items of $X$ and $Y$ that has no cost.
	\State ~~~~~~ Do not add them to $G$.
	\State ~~~~ Inserting auxiliary nodes if the $|X| \neq |Y|$


	\State Start with $M$ equal to the empty set
	\State Define $p(x)$ for $x \in X$, and  $p(y) = \underset{e \; into \; y}{\operatorname{min}} c_e$ for $y \in Y$
	\While{$M$ is not a perfect matching}
    	\State Find a minimum-cost $s-t$ path $P$ in $G_M$ with prices $p$
    	\State Augment along $P$ to produce a new matching $M'$
    	\State Find a set of compatible prices with respect to $M'$
    \EndWhile
    

	\Return the union of three sets:
    \State ~~~~ For matchings with an auxiliary node, select $(x, \epsilon)$ or $(\epsilon, y)$
    \State ~~~~ For other matchings select the matching.
    \State ~~~~ For each item not added to $G$, select $(x, \epsilon)$ or $(\epsilon, y)$    
\EndFunction
\end{algorithmic}
\caption{Two-way set matching algorithm}
\label{TwoWayCostMatching}
\end{algorithm}


\subsubsection{Three-way set matching with costs}
When we are given an original set $O$ and two branched sets $A$ and $B$, three way cost-matching allows us to decided what items of each set belong together if we define a decent cost-function. We sort the output matching after the index of their input $O$-items, to align all items that have been matched in all three sets. Then we then iterate through the sequences, and add them as unmatched in their opposite branch, or add the matching from both branches to the result list. Algorithm \ref{ThreeWayCostMatchingAlgorithm} shows this.

\paragraph{Running time} The two two-way cost matching algorithms runs in $O(m_{ao}^2 \log m_{ao})$ and $O(m_{bo}^2 \log m_{bo})$ time, where $m_{ao}$ is $max(|A|, |O|)$ and $m_{bo}$ is $max(|B|, |O|)$. They return sequences of the maximal sizes $A+O$ and $O+B$. Ordering each of the matchings takes $O(n \log n)$ time; which overall gives $O((A+O) \log (A+O) + (B+O) \log (B+O))$. Afterwards we iterate through $A+O+B$ items, and for each of them do constant time operations.

This gives us the overall time of:
\begin{equation}
O(m_{ao}^2 \log m_{ao} + m_{bo}^2 \log m_{bo} + (A+O) \log (A+O) + (B+O) \log (B+O) + A + O + B) \nonumber
\end{equation}

If we define $n$ as $max(|A|, |O|, |B|)$, which is clearly an over-approximation, the expression can be simplified to:

\begin{equation}
O(n^2 \log n + n \log (n) + n) \nonumber
\end{equation}

And further to:

\begin{equation}
O(n^2 \log n) \nonumber
\end{equation}


\begin{algorithm}
\begin{algorithmic}
\Function{ThreeWayCostMatching}{$A$, $O$, $B$, $cost$}
	\State $ma\gets TwoWayCostMatch(A, O, cost)$
	\State $mb\gets TwoWayCostMatch(B, O, cost)$
	
	\State Sort $ma$ after the index of $O$-items
	\State Sort $mb$ after the index of $O$-items

	\State $a \gets 0$
	\State $b \gets 0$
	
	\State $rv \gets []$
	
	\While{$a+b < |ma| + |mb|$}
		\If{$a \geq |ma|$ or $b \geq |mb|$}
			\State add item from opposite matching to $rv$
			\State increase opposite counter
		\ElsIf{$ma[a].o$ is $null$ and $mb[b].o$ is $null$}
			\State add the $ma[a]$ to $rv$
			\State add the $mb[b]$ to $rv$
			\State increase $a$ and $b$
		\Else
			\State add $(ma[a], mb[b])$ to $rv$
			\State increase $a$ and $b$
		\EndIf
	\EndWhile
	\State \Return $rv$
\EndFunction
\end{algorithmic}
\caption{Three-way set matching algorithm}
  \label{ThreeWayCostMatchingAlgorithm}
\end{algorithm}

\subsubsection{Reordering}
\label{ThreeWayReorderingAlgorithmSec}
Several times in this thesis, we match sets together like they have no order, and afterwards want to produce a sequence for our output. Here we provide a function to resolve the problem where reordering of items in a sequence have happened in different branches. We define a general three-way sequence-reordering-function, that can be given an original sequence and two reordered sequences, the result is a single sequence where all elements are outputted in a way that can reasonably be described as a merge of the orders in the three input sequences.

Reorderings can conflict. One example could be that the sequence \texttt{(a, b, x, c, d)} has turned into \texttt{(x, a, b, c, d)} in one branch and \texttt{(a, b, c, d, x)}. These three sequences can easily be intepreted as \texttt{x} being been moved different places in the two branches, and a conflict should be produced.

If one branch moves an item and the other branch deletes it, we interpret this as a deletion. The intuition behind this, is that generally unordered sequences in programming languages are semantically meaningless. Therefore if one user deleted it while another user moved it, chances are that the item is not needed anymore, and can be deleted.

To do a three-way reordering, the obvious entry point is to identify which items have been moved, and then apply all these moves to the output sequence. This however leads to quite a few non-trivial definitions. What is a move? It might very well be an item of the sequences, but how do we define the place that it should be moved to? Several options are available: Insert after or before a specific item, insert between items or insert at index, and many more. Each of these options, however present further complications. If we choose to insert something after an item, in the output sequence, then do we do that before or after we move that item?

Further, such a move operation is not our input, and it is not clear how to calculate one from our input. Our input is three sequences with a specific order, and given those, many different move operations could be generated. The two sequences \texttt{a, b, c, d} and \texttt{a, b, d, c} can be interpreted as \texttt{c} being moved to after \texttt{d}, as \texttt{d} being placed before \texttt{c}, as \texttt{c} and \texttt{d} swapping places, and many other options. Also, figuring out the correct representation of a move in an algorithmic way can only be an uninformed guess - the users reasoning for doing a reordering might very well be grounded in context that the algorithm is unaware of.

We interpret reorderings differently. Given the input sequences we do three-way matching. Such a matching produces the actual ordering. A matching contain some stable elements and some unstable elements. The stable elements are elements where a match exists in both A, O and B. These elements we assume to be items that have not moved in the sequence. If something is not stable, it is either a move, an insertion or a deletion as seen in table \ref{ReorderingTable}.

\begin{figure}
\centering
\begin{tabular}{ | l | l | l || r |}
  \hline                        
   \textbf{A} & \textbf{O} & \textbf{B} & \textbf{Action by user} \\
  \hline                        
  Empty & Value & Value & "Moved from" or deleted in A \\
  Value & Value & Empty & "Moved from" or deleted in B \\
  Value & Empty & Empty & Inserted or "moved to" in A \\
  Empty & Empty & Value & Inserted or "moved to" in B \\
  Value & Value & Value & Unchanged \\
  \hline  
\end{tabular}
  \caption{How to interpret matchings in the reordering algorithm}
\label{ReorderingTable}
\end{figure}

We iterate through each matching, and treat them as such. Distinguishing between a deletion or a "move from" is not important - we treat them the same, and do not add them to the output. Distinguishing between insertion and moving is quite important however. Insertions should always be inserted in the output, however moves should only be inserted to the output if they have not been deleted in the opposite branch. 

A conflicting move is a move where two "Moved to" operations exists on each list. Detecting these is a matter of looking into duplicate records in the result list. These are returned along with the result, and the callers of the reordering methods have the responsibility of presenting these to the user.

\begin{figure}
   \centerline{\includegraphics[scale=0.55]{drawings/eps/reordering.eps}}
   \caption{Two possible outcomes of the reordering algorithm. Gray items in the result are conflicts.}
   \label{Reordering}
\end{figure}

The process of reordering can be seen in Figure \ref{Reordering}. This figure also shows that the ambiguity in our two-way-matching algorithm can lead to different potential outcomes of the reordering algorithm; if the $A \rightarrow O$-matching chooses to match the 4; implying that 4 has only moved in the $O \rightarrow B$-match, we produce a conflict-free output. If the $A \rightarrow O$-matching chooses 3 as the match, we get a conflict where 4 is being inserted both places. The safe bet is to modify the algorithm such that a conflict is always produced, by modifying the two-way-matching to return all possible matchings, and let the three-way-matching algorithm choose a pair that share most matchings.

\paragraph{Running time} ThreeWayMatching runs in $O(|A||O| t + |B||O| t + |A|+|O|+|B| )$ time, and in worst case it returns $|A|+|O|+|B|$ items. The loop in the algorithm only has constant time operations which means the overall running time is $O(|A|+|O|+|B|)$ for the loop. Conflict detection can be done by keeping a hash set on the side of the results already added which gives us constant time lookup for each item; leaving us with $O(|A|+|O|+|B|)$ time.

The reordering algorithm thereby has the following runtime:

\begin{equation}
O(|A||O| t + |B||O| t + |A|+|O|+|B| ) \nonumber
\end{equation}

\begin{algorithm}
\begin{algorithmic}
\Function{Reordering}{$A$, $O$, $B$}
   \State $matches \gets ThreeWayMatcing(A, O, B, items equal)$
   \State $resultList \gets []$
   
   \ForAll{$match$ in $matches$}
      \If{$match$ is insertion}
         \State add relevant item (A or B) to $resultList$.
      \EndIf
      \If{$match$ is move and exists in opposite branch}
         \State add relevant item (A or B) to $resultList$.
      \EndIf
      \If{$match$ is unchanged}
         \State add item O to $resultList$
      \EndIf
      
	\EndFor
	\State $conflicts \gets$ find indexes of duplicate items in $resultList$
	\State \Return $(resultList, conflicts)$
\EndFunction
\end{algorithmic}
\caption{Three-way reordering algorithm}
  \label{ThreeWayReorderingAlgorithm}
\end{algorithm}

\subsection{Merging files}
Merging files is initially be done using a line-based approach, and when a conflict is detected, we break down the syntax tree and use a structured merged instead. In this section we briefly describe our algorithms for the line based approach and for conflict detection and resolving.

\subsubsection{Sequence based merging}
The sequence based merging algorithm in figure \ref{Genericmergingalgorithm} is based on the description found of the original $diff3$ algorithm in \citet{Khanna}. Conflict-handling is delegated to an input function, to allow different handling in different situations. We pass the list of current merged text, and the currently unmerged chunk. The merged list can be arbitrarily modified inside the conflict handler. Also, We allow the conflict-handler to break out of the entire loop. This allows two essential use-cases:

\begin{itemize}
   \item Output the three items of the chunk, indicating that these are conflicting.
   \item Clear the merged list inside the conflicthandler, and start over with a different merging algorithm.
\end{itemize}

\paragraph{Running time} Three-way matching and chunking gives us an overall running time of $O(|A||O| t + |B||O| t + |A|+|O|+|B|)$, given that $t$ is the running time of $equality$. The loop runs $|A|+|O|+|B|$ times. All operations inside the loop are constant time, except for the conflict-handler which has an unknown running time. The above two use cases can also be divided into two scenarios for the conflict-handler:

\begin{itemize}
	\item When the conflict-handler output the chunks as conflicting, it runs only constant time operations on the chunks, for any chunk. The running time is:\\
		\begin{equation}
			O(|A||O| t + |B||O| t + |A|+|O|+|B| ) \nonumber
		\end{equation}

	
	\item When the conflict-handler flushes the output and starts a new merging algorithm it also break the loop, meaning conflicthandler is only invoked once. Given that $t'$ is the time it takes to run the conflicthandler, the running time is:\\
		\begin{equation}
			O(|A||O| t + |B||O| t + |A|+|O|+|B| + t') \nonumber
		\end{equation}

\end{itemize}


\begin{algorithm}
\begin{algorithmic}
\Function{Merge}{$A$, $O$, $B$, $equality$, $comparer$}
   \State $match \gets ThreeWayMatch(A, O, B, equality)$
   \State $chunks \gets getChunks(match, comparer)$
   \State $merged \gets []$
   \ForAll{$chunk$ in $chunks$}
        \If {$chunk$ is stable}
            \State add line of chunk.O to merged
            \State continue to next iteration
        \EndIf
        \If {$chunk$ is added in $A$ or $B$}
            \State add lines of $chunk$ to $merged$
        \Else
            \If{$O$ is equal with one branch, but not the other}
               \State add lines of changed branch to $merged$
            \Else
                \If{the two branches are equal}
                   \State add either of them to merged
                \Else
                   \State $conflicthandler(merged, chunk)$
                   \If{$conflicthandler$ indicates termination}
                      \State break
                   \EndIf
                \EndIf
			\EndIf
   		\EndIf
    \EndFor
	\State \Return $merged$
\EndFunction


\end{algorithmic}
\caption{Merging algorithm}
  \label{Genericmergingalgorithm}
\end{algorithm}

\subsubsection{Mergefile function}
This $MergeFile$-function in Algorithm \ref{Mainmerge} does a line-based merge. It does a syntactic merge if a conflict is detected in the file.

The main merging function of this thesis consists of a conflict-handler and a general merge function. The merge function is passed in three lists of strings. Each item in this list represents a line in the code-files we want to merge. It pass these lists to the to the sequence merging algorithm, and if this algorithm encounters a conflict, the conflicthandler is called. The conflict-handler flushes the output, and launch the Syntax-aware merging, and add this as output instead. Then it indicates to the merging algorithm that the entire input sequence is merged.

\paragraph{Running time} The running time for this is the second scenario of the sequence based merging algorithm, with the equality function being constant time. The running time for $t'$ is defined later. 
		\begin{equation}
			O(|A||O| + |B||O| + |A|+|O|+|B| + t') \nonumber
		\end{equation}


\begin{algorithm}
\begin{algorithmic}
\Function{MergeFile}{$A$, $O$, $B$}
   \State $Conflicthandler \gets (output, chunk) \Rightarrow$
      \State ~~~~~~~~~~~~~~~~~~~~ clear $output$
      \State ~~~~~~~~~~~~~~~~~~~~ let $output$ be the result of $MergeSyntax(A, O, B)$
      \State ~~~~~~~~~~~~~~~~~~~~ terminate outer merge function
	\State $equality \gets (x, y) \Rightarrow x = y$
	\State \Return $Merge(A, O, B, equality, conflicthandler)$
\EndFunction
\end{algorithmic}
\caption{File-merge algorithm}
  \label{Mainmerge}
\end{algorithm}

\subsection{Syntactic breakdown}
At some point we have to break down the files from text to syntax trees. To do this we'll use Microsofts Roslyn Compiler. It has implemented a C\# syntax tree as a .NET class hierarchy, and with a single function call it is possible to get a syntax tree produced from a code file. This syntax tree seems to be very close corresponding to the actual grammatical rules set up by it's languages.

\todo[inline]{Write more about this}
\todo[inline]{Why C\#?}

\paragraph{Running time} The running time for getting syntax trees from Roslyn does not seem to be specified. However, Visual Studio relies on Roslyn for compilation, refactoring and code highlighting which all seems reasonable fast.

\subsection{Merging classes}
\label{mergingclasses}
The top level of a C\# file contains an optional sequence of using-statements followed by a sequence of namespaces or type-declaration. A type-declaration is a struct, a class, an enum, a delegate or an interface. Classes, structs and namespaces can be nested in each other.

This structure in itself can lead to many potential merge scenarios; members of types can be moved and types can be nested in different way. We look aside most of these issues and focus on merging purely inside classes. This means that a move of a class member looks like a deletion at one place, and an insertion at another place. As a consequence, a change in a function that is also moved moved between structures therefore produces a conflict.

We do an unordered match between the the base file, and the two input brances using heuristics to determine the match of the input trees children. Nodes that are matched in the syntax trees are merged together. The overall process is:

\begin{itemize}
   \item Match similar namespaces together and merge these.
   \item During a merge of namespaces, match similar classes together and merge these.
   \item During the merge of classes, match similar class-members and merge these.
\end{itemize}

The input for the Algorithm \ref{TreeMergeAlgorithm} is three syntax tree generated from the input files. The output is a text string with the concrete syntax of the merge. Whenever the function encounters a namespace or a class, it does an unordered matching on these, and recursively call itself for each match. Whenever a class-member is hit in the recursive call, we try a line-based merge. If a conflict happens during this merge, we launch a syntax based merge of the specific class-member. The effect of this is, that class-members only does expensive syntax merge as a last resort.

After creating the merge of all children for a class or a namespace, we need to create the ordered sequence of the merged items, that can be used to generate the entire merged node. We store all matches in a list, along with their merge-result. From this list we generate three new sequences for each input sequence. These lists are feeded into the reordering function seen in section \ref{ThreeWayReorderingAlgorithmSec}.

Items that are deleted in one branch and modified in another branch, should generate a conflict. The reordering function assumes equality between the items in the sequences, and therefore cannot consider a deletion to be a possible conflict. Therefore we detect conflicts during merging and pass a set of these to a modified Reordering function, that only deletes items if they are not a part of this set.

\paragraph{Running time} $f(x)$ is the total number of nodes that is found in the tree from the root, down to until the leaves that are class member; it contain all root-items, namespaces, classes and class members, but no nodes from below class-members. $n$ is $max(f(A) f(O), f(B))$ and the time for running the cost function is constant. 

The running time for the match is $O(n^2 \log n)$, and it returns $O(n)$ matchings, giving an overall upper bound of the running time of the loop as $O((n^2 \log n)^n)$. This upper is awful; yet two factors allow us to tolerate this. First, it is a very crude upper bound that illustrates the situation where every node contains the entire tree which is obviously not the case. Second, the number of namespaces, classes and class-members per file is often quite low.

\begin{algorithm}
\begin{algorithmic}
\Function{MergeSyntax}{$A$, $O$, $B$}
    \If{nodes are roots, namespaces or classes}
        \State $zip  \gets ThreeWayCostMatching(A.children, O.children,$
        \State ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  $B.children,$ relevant cost function $)$  
        \State $members \gets [];$
        \State $deletionconflicts \gets []$
        \ForAll{$m$ in $zip$}
           \If{$m$ exists in all}
               \State $member.add(m, MergeSyntax(m.A, m.O, m.B))$
           \ElsIf{match was inserted in A or B}
              \State $member.add(m, (m.A$ or $m.B ))$
           \ElsIf{match was deleted and changed in opposite branch}
               \State $member.add(m, (m.A$ or $m.B ))$
               \State $deletionconflicts.add(m.A$ or $m.B )$
           \EndIf
           \State $reorderLists \gets $ map each item in each list (A, O, B)
           \State ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ to their merged result.
           \State $members = Reorder(A, O, B, conflicts)$
       \EndFor
       \State \Return $recreate(b, l, r, members)$
    \ElsIf{nodes are classmembers}
       \State Run $MergeFile$ on the source text of A, O, B
       \State If a conflict happens run $MergeNode$ on the nodes.
    \EndIf
\EndFunction
\end{algorithmic}
\caption{Class-merging algorithm}
\label{TreeMergeAlgorithm}
\end{algorithm}

\subsubsection{Cost-functions}
We have to define the cost-functions for namespaces, classes and class-members. For each level, these functions are called for each pair of items possible in $(A, O)$ and $(O, B)$. As a consequence, we need to keep the runtime cost of calculating a cost of two items relatively low.

For classes and namespaces we define functions that assign low cost to nodes with similar identifiers, and a higher cost to all other nodes of the same type on the same structural level. This approach has the benefit of being quite fast. Further, renaming a class on the same structural level match correctly, since it is matched to another node of the same type. A drawback, however, is that given this heuristic, we have no way to know what to do if a node has been renamed and another node inserted at the same time: All possible matchings in such case are valid and has the same cost. A better heuristic for namespaces and classes probably is to also include the different members of namespaces and classes as a part of the cost-function, however we decided not pursue this approach further.

One obvious function to use to assign the cost between two functions, is the similarity function defined in section \ref{FunctionSimilarity}. However this function is quite intensive computation-wise, since it walks through the entire tree. This is expensive computation time-wise, and since it is done for every possible pair it leads to an unfortunate explosion of running times. Instead we limit our matching to the signature of a method:

\begin{itemize}
    \item The return type.
    \item The identifier.
    \item The type and identifier for each parameter.
\end{itemize}

The more of these factors that are similar, the lower a cost two functions should have. Matching functions that are very dissimilar in their method body might produce unproductively numbers of conflicts, and because of this it's a goal of this heuristic to not match things together too greedily.

Equal identifiers is a clear sign of similarity, and one that should weight heigh for this scale. Parameters are also important; we need to match different overloads of a function to the same overload in another branch. Given these two criterias we match any unchanged function completely.

If we look at two functions with dissimilar names, equal return types and no parameterlist, it is not quite obvious how to define if they are similar or not. For this reason we define it as not similar. However with several parameters similar, the likelihood of the functions being the same increases.  The cost-function can be found in Algorithm \ref{MethodCostFunction}.

\begin{algorithm}
\caption{Cost for method-similarity}
\label{MethodCostFunction}
\begin{algorithmic}
\Function{FunctionCost}{x, y}
   \State $parametersimilarity \gets 0$
   \If{parameterlists of $x$ and $y$ are not empty}
      \State match parameters after having either identifier or type equal.
      \ForAll{$match$}
	      \State select 1 if both name and type are equal
    	  \State select 0.5 if either name or type are equal
	      \State select 0 if there is no matching item
      \EndFor
      \State $parametersimilarity \gets 4*the averager of above match value$
   \EndIf
   \State $cost \gets 9$
   \If{equal return type of $x$ and $y$}
      \State $cost \gets cost - 1$
   \EndIf
   \If{equal identifiers}
      \State $cost \gets cost - 4$
   \EndIf
   \State $cost \gets cost - parametersimilarity$
   \If{$cost \geq 4$}
         \State \Return No link
   \EndIf
   \State \Return $cost$
\EndFunction
\end{algorithmic}
\end{algorithm}

This function has some properties that are helpful, given the above considerations:

\begin{enumerate}
\item If the name of a function is equal, a possible match is produced.
\item Higher similarity between parameters gives better costs.
\item Renamed functions can be matched if they match completely in the parameterslist, or if some of the parameterlist is similar and it has the same return type.
\end{enumerate}


\subsection{Merging functions}

%\todo[inline]{Foreach function, write about:
%
%- Intended input. \\
%- Purpose of the function.\\
%- Conflict behaviour\\
%- Tricks that are not obvious.\\
%- Intended output.\\}

\subsubsection{Merging nodes}
\texttt{MergeNode} handles merging of single nodes of equal type. It takes three input values; A, O and B, and return a syntactically valid string, which is the merge of those three input values. It handles the input as a matching and concludes insertion or deletion if appropriate. Further it assumes that all input nodes are of the same type.

\begin{itemize}
   \item If O is null and either A or B is null it treats the input as an insertion, and returns the inserted value.
   \item If O is not null, and A or B is null it treats the input as a deletion. It verifies that no changes has happened in the other branch and return either a conflict, or empty.
\end{itemize}

Otherwise it treats the input as a merge of the two branches A and B. From a concrete syntax perspective, we look into two factors of the current node, to see what to return:

\begin{itemize}
   \item The concrete syntax that this node emits.
   \item The concrete syntax that each child of this node emits.
\end{itemize}

The responsibility of an invocation of MergeNode is to return the the exact syntax needed for these specific node. Any work of generating concrete syntax for children is handled by a recursive call to MergeNode. For example a method invocation generate the opening and closing parenthesis, while the argument list be passed to another invocation, as well as the method-expression. This breakdown makes the merging of most nodes trivial.

Some nodes only consists of tokens: A function name, a string literal, an integer, a keyword and so forth. For these kind of nodes, we check if only one of the branch nodes differs, and if this is the case, return that, and otherwise return a conflict, as can be seen in Figure \ref{MergeToken}.

\todo[inline]{A fixed structural node is nodes that has a fixed number of children, that are easily merged - like methods declerations, expressionsstatements, invocations, memberaccesses, single arguments. A content node is a node which is generated from a single token.}


\begin{algorithm}
\caption{Tree-merging algorithm}
\label{MergeNode}
\begin{algorithmic}
\Function{MergeNode}{$A$, $O$, $B$}
    \If{$A$ or $B$ has been deleted}
        \If{others havent changed}
            \State \Return nothing;
        \Else
            \State \Return conflict;
        \EndIf
    \EndIf
    \If{$A$ or $B$ has been inserted}
        \State \Return $A$ or $B$
    \EndIf
    \If{nodes are fixed structural nodes}
        \State \Return merge of each child and own structure
    \EndIf
    \If{node is a content-node}
        \State \Return $TokenMerge(A, O, B)$
    \EndIf
    \If{nodes is a list}
        \State \Return $Listmerger(A, O, B, $ appropriate cost function $)$
    \EndIf
    \If{node is a block}
        \State \Return $MergeStatementList(A.children, O.children, B.children)$
    \EndIf
\EndFunction
\end{algorithmic}
\end{algorithm}


\begin{algorithm}
\begin{algorithmic}
\Function{MergeToken}{$A$, $O$, $B$}
    \If{all equal}
        \State \Return value
    \EndIf
    \If{one has changed}
        \State \Return changed value
    \EndIf
    \If{both has changed}
        \If{$A \neq B$}
            \State \Return conflict
	    \EndIf
        \State \Return A
    \EndIf
\EndFunction
\end{algorithmic}
  \caption{Token merging}
  \label{MergeToken}
\end{algorithm}

\subsubsection{Equality}
The equality function defines whether or not two nodes taken from syntax-trees in different branches are equal to each other.

If we view syntax trees as simple n-ary trees with no semantic meaning, this becomes easy. Every node has a label and $n$ children. Given two nodes $x$ and $y$ we need to check if the label is equal, and we need to iterate through each pair of children on the same index. Whenever we encounter something not equal, return false. Otherwise return true.

\subsubsection{Similarity}
\label{FunctionSimilarity}
Similarity should return whether two nodes are similar or not. We divide it into two functions:  One that decides a similarity percentage between two nodes. Around this function we define a function that thresholds this percentage.

\paragraph{Initial assumptions} Is the expression $x=1+2+3$ similar to $f(1+2+3)$? They have atleast 5 nodes in common; three integers and two add operators - this, out of 7 nodes in total. From that perspective they seem quite similar. For several reasons, we decide that only syntax-nodes of equal type should be considered similar:

\begin{enumerate}
\item As noted by \citet{Hashimoto}, showing a fine grained difference, might be more confusing than showing the two statements as an insertion and deletion.
\item The running time of creating the matchings become considerably larger if every single sub-tree has to be compared to all other sub-trees  for similarity.
\item The complexity of the merge algorithm is considerably less, if we can assume that children of any of the input nodes are always the same. This is also the case for the similarity function itself.
\item If we want to allow for merging of the above cases, this can still be done, in the same way that we allow of uneven tree merging when no other ways of resolving an update is possible: It generates an unstable chunk. Given this chunk we can match the sub-trees much quicker because we have a much more narrow field for the matching.
\end{enumerate}

Given that two nodes are only equal if they are of the same node-type, most of the similarity is quite straightforward: Run similarity on all children, and do an average of these. This is however not possible for nodes that contains lists of sub-nodes - blocks, parameter-lists and argument-lists.

\paragraph{Lists} Parameters and arguments throughout the syntax tree are placed in lists. Finding similarity between these lists, is a question of matching the arguments or the parameters inside the lists. This matching is done without order: If the order of parameters or arguments have changed across branches, it is probably more due to reorderings or change in overloads, than it has anything to do with change in actual behaviour.

\paragraph{Blocks} Similarity for blocks is determined by doing a priority-chunk-matching, and for each chunk, count up by adding up the similarity and the count of statements:

\begin{itemize}
	\item For equality-stable chunks we add 1 to the similarity and 1 to the statement-count foreach statement
	\item For similarity-stable chunks we add the similarity between the two statement, and 1 to the statement-count for each matching
	\item For unstable chunks we add nothing to the similarity and the maximum number of statement in both of the sides for the unstable chunk.
\end{itemize}

The first two are obvious: Together they are used to calculate the average similarity across statements. Adding nothing to similarity for the last item, is also pretty obvious. Choosing how many statements to add is however not - one way to look at it is that each of the statements in the unstable chunk counts. This, however, means that a statements in unstable chunks have double the weight in the average compared to stable chunks: Two statements that are not similar both counts, while two statements that are similar only count once.

Because of this we have choosen just the maximum of the two counts - this is the upper bound on how many dissimilar statements is in the block.

\paragraph{Token similarity} Tokens-similarity is primarily merge of variable-names, integers, booleans and string. For booleans and integers strict equality of the token is the only sensible choice. For strings and variables there are many ways to compare them heuristically. However, since our goal for matching is to merge the nodes, it seems quite nonsensical to start using heuristics for such a matching: We cannot automatically merge two strings or variables, since we do not know the structure of them - it is better to leave them unmatched so they can be handled as an unstable chunks.

\paragraph{Threshold} When the similarity of two nodes is calculated, we have to answer whether or not they are similar \textit{enough}. We set the cut off around 60\% equality.

\todo[inline]{Write more about cutoff}

\subsubsection{Merging lists}
There are primarily two types of list that we want to merge: ParameterLists and ArgumentLists. Parameterlists are the pair of types and identifiers found in method declarations, where argument-lists are expressions found in invocations of methods. Modification of these two properties 

\begin{itemize}
   \item Modifications of parameter-lists can be reorderings with no semantic meaning. Further it can be additions or removals of a parameter. These are often accompanied by some change in the behaviour of a method.
   \item Modifications of argument-lists can be done as a consequence of change in a parameter list other places in the syntax-tree. Modifications in arguments can also be to change the expression or value to be passed into a function, or because the user intends to call another overload of the method.
\end{itemize}

The first part is solved the same way that we reorderings were solved in section \todo{page number}. We do an unordered matching of them, and then do a reordering, and filtering of the output. The returned list is a list that tries to preserve any reordering, and that has filtered items in the sequence that did not exist in either of the two branches. This hopefully ensures that:

\begin{itemize}
   \item Since the reordering uses the exact same algorithm, merging both parameter- and argument-lists produces compatible output, given that the cost function matches in the same way on both types of lists.
   \item All items that existed in the branches exist in the output of this function. 
\end{itemize}

The cost between a pair of parameters can be boiled down the question of how similar the type of the parameter is, and how similar the name is. Similarity of type can be measured as closeness in the class hierarchy. String similarity can be measured in quite a few ways. In our case we want to keep the runtime low. Therefore we keep to a simple and exact scheme of similarity of both.

The cost of the argument-lists is much harder to define. An argument is an expression, so the obvious ideas is to look into what type that expression has. This information, however, is not available. Instead we look into the string value of the nodes and compare these for equality or not. Lower equality means lower cost. This cost-function produces matchings of items that have not been changed, but have been reordered, and items that have changed are not matched; thereby they are treated as insertions, deletions or moves.

The first item in the above list, depends on the matchings in both parameterlists and argumentlists to match the same items. This is not generally true for for the two matching functions that we provide, but it is true in cases where reordering have happened without changing the expressions of each parameter.

\begin{algorithm}
  \caption{Unordered list merging algorithm}
  \label{Listmerger}
\begin{algorithmic}
\Function{Listmerger}{A, O, B, cost}
    \State $match \gets ThreeWayCostMatching(A, O, B, cost)$
    \State $(merge, conflicts) \gets FilterReorderMergeContent(a, o, b, match)$
    \ForAll{conflict}
        \State Add a warning to the merges.
    \EndFor
    \State \Return $merge$
\EndFunction
\end{algorithmic}
\end{algorithm}

\todo[inline]{FilterReorderMergeContent}

\subsubsection{Merging blocks}
A block-node is hit by the MergeNode algorithm often. They are the body of methods, if-statements, while-loops, using-statements and more. We need to merge content inside in each branch and the base. We do a textual merge of the content of the block. If this fails, we do a priority-differencing as described in section \ref{PriorityDiff}, with the equality and and similarity function defined in algorithm \ref{FunctionSimilarity}.

Given these chunks we handle them in three different ways: 

\begin{itemize}
	\item If a chunk is Equality-stable use the input from the base tree.
	\item If a chunk is Similarity-stable, merge each node using $MergeNode$
	\item If a chunk is unstable, check of either branch is equal with base, and use the other.
	\item Otherwise try to do an uneven merge as described later.
\end{itemize}

\begin{algorithm}
  \caption{Statement-list merging algorithm}
  \label{MergeStatementList}
\begin{algorithmic}
\Function{MergeStatementList}{A, O, B}
    \State try  a textual merge of the statements.
    \If{that does not work}
        \State $chunk \gets PriorityDiff(A, O, B, equality, similarity)$
        \ForAll{chunks}
            \If{chunk is primaryequal}
                \State \Return $chunk.O$
            \ElsIf{chunk is secondary equal}
                \State for each line \Return $MergeNode(A, O, B)$
            \ElsIf{either A and O or O and B are equal}
                \State add the opposite chunk.
            \Else
                \State attempt uneven merging
            \EndIf
        \EndFor
    \EndIf
\EndFunction
\end{algorithmic}
\end{algorithm}

\subsection{Merging uneven trees}
Generally our merge has been working with structural equality. This means that if one branch moved a chunk of code into a block, and the opposite branch changed something in the chunk of code, we have no way of handling this.

$MergeUneven$ is called in case of an unsolvable conflict in Algorithm \ref{MergeUneven}, and tries to handle the scenario of inserted sub-block in the code - for example wrapping a couple of lines of code in an if-statement. It takes an unstable chunk as input, that contains a sequence of nodes that cannot be matched. MergeUneven looks for nodes in the chunks that contains sub-blocks which might create a stable match. It works as can be seen in Figure \ref{UeventreeProcess}. Below is a description of each step.

\paragraph{Flattening} $MergeUneven$ starts by flatting the input lists, in the sense that for any node found in the input lists that contains a sub-statement or a block, we bring these statements into the top level. During the flattening we remember the parent of each node, as can be seen in the left columns of the Flattened-item in the figure. Looking at the flattened list of child and parent elements,  it is obvious that chunks of continuous parent items in any of the three lists is a sub-block.

\paragraph{Matching} Afterwards we run $PriorityChunking$ on the child elements of the flattened lists. If some statements have been moved to a subscope for these conflicts, this matches these statements with the ones in the other branches scope. As mentioned above, a continuous sequence of keys in the three branches means a sub-block. The same is the case in this matching: For each branch, we are able to track opening and closing sub-scopes by looking at the change between the last observed parent element, and at the current. This means we can isolate chunks of matches that happen in specific scopes. After isolating these, we can merge the parents, and for each continuous sequence of parent items, we can output a block around them.

\begin{figure}
   \centerline{\includegraphics[scale=0.55]{drawings/html/EditedFlattenedMerge.eps}}
   \caption{Process of merging uneven trees.}
   \label{UeventreeProcess}
\end{figure}

\begin{algorithm}
\begin{algorithmic}
\Function{MergeUneven}{A, O, B}
    \State Flatten the three list. Any statement with a subblock, has the statements in this subblock added to it, keeping parent item mapping
    \State Prioritydiff the three flattened lists.

    \State Parentitems = [nil, nil, nil]
    \ForAll{$chunk$ in $chunks$}
        \If{$chunk$ is stable}
            \If{lastitem O is null and B and A is not}
                \State \Return conflict
			\EndIf
            \If{any parentitem has changed since last iteration}
                \If{last parent item as a block block}
                    \State print "\}"
                \EndIf
                \If{a current parent item exists}
                    \State print(merge(parentitems))
                    \If{currentparentitems merrits block}
                    	\State print "\{"
                    \EndIf
                \EndIf
			\EndIf
            \State print merge(child item)

        \ElsIf{$A$ or $B$ is insertion}
                \State add A or B, updating their corresponding parentitem,
                \State and adding starting and ending items, as above
        \EndIf
   \EndFor

\EndFunction
\end{algorithmic}
\caption{Uneven branch merging algorithm}
\label{MergeUneven}
\end{algorithm}


\clearpage
\section{Future work}

\clearpage
\section{Conclusion}

\clearpage

\listofalgorithms

\clearpage



\bibliographystyle{plainnat}
\bibliography{libary}

\end{document}

