\title{Theory}

\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\author{
        Kasper Videbæk \\
}
\date{\today}
\usepackage[numbers]{natbib}
\usepackage{todonotes}
\usepackage[hidelinks]{hyperref}

\begin{document}
\maketitle

\tableofcontents

\clearpage 
\section{Introduction}

\clearpage 
\section{Theory}
Several topics are related to the work that will be done in this thesis. In this section I will give a brief overview of what areas we will be going through, and what related work has already been done in those areas. In the first subsection we will look into research that already have worked on embedding syntax tree knowledge into version control systems, and in the second section, we will look into tree differencing. In the last section we will look into general knowledge about merging trees.

\todo[inline]{Syntax Trees and Preprocessing??}
%http://bartho.net/publications/Adding_preprocessor_directives.pdf


\subsection{Automatic merging}
Merging in version controls is the process of producing a single file, given two files that have evolved in different directions from a common ancestor. Merging source code is an inherent part of software development, and has gone through some development and automation since the first version control systems. Merging algorithms generally work by matching content between the two branches and the common ancestor. Given these matchings it is possible to merge the content into one common child. If this is not possible a conflict is detected and presented to the user.

Merging tools of version control systems are mostly line based and oblivious to the structure of the content of the files contained. This can probably attributed to such an approach working relatively well with source code, XML and other files often found in repositories.

Structure matching will most definitely also be a more time consuming task, and given the general use case of some commits, it might not be worth the extra computation time. Further programs are generally undecidable which means it would be impossible to actually decide if a merge will work as intended by the user, without the user assessing it manually.

One very commonly used algorithm for merging is Diff3. \citet{Khanna} investigated it formally by regarding a file as a sequence where each line was an item in the sequence. They set up a number of expectations of what you would expect from a merge, and showed that many of these assumptions does not hold: Conflicts can happen even when only unrelated regions are changed, differencing after a merge does not produce empty output, merging fails when both branches are very different from the ancestor and so forth. Even though this is the case, Diff3 continues to be widely used with few complaints of it shortcomings - it seems that the situations where Diff3 breaks are quite uncommon in version control systems.

Common algorithms for merging are state based. They compare the final state of branches, and creating a series of operations that will transform one document from one state into another. \citet{Lippe} described a different approach, where editing tools instead would remember the operations used to transform the data, and where the merge process will be a matter of applying these transformations in the right order to minimize the amount of conflicts.
\todo[inline]{Mention whitespace ignoring}
\todo[inline]{Mention horizontal differincing}
\todo[inline]{Talk about prettyprinters.}
\todo[inline]{Point out that merges might not compile.}

\subsection{Syntax Trees in Version Control}
There has been several attempts at creating more structure awareness in version control systems with several different goals. \citet{Freese} describes the idea of version control systems that would understand the code in a repository, and also have capabilities of migrating newly committed code automatically when a public API has changed.

Syntax tree differencing has generally been used to get overview of changes in version controls. \citet{Fluri} describes an approximating algorithm for trees, that  detects moves and and uses a heuristics approach to define how good a match a node is to another node - looking at both the value but also the child nodes. They also describe string similarity measures in which reordering of words in variable names will be more similar and describe use cases for this. \citet{Hashimoto} also describes the idea of analysing code through syntax trees and discuss several applications. They presents an approach that works for four different object-oriented languages, and note that too fine-grained tree-edit-scripts will actually be less comprehensible than more coarse-grained.

\citet{Hunt} describes the idea of using renaming and move-detection to avoid conflicts in language aware merging tools, and the idea of redefining the definition of conflicts to include some semantic conflicts, where the user should be warned about branches interfering in each others code. They do not provide much low level information about the algorithm.

\citet{Ekman} also looks into refactorings using the ideas of \citet{Lippe}. The idea is to log refactorings and normal code changes, they apply traditional changes first and thereafter apply refactorings. Further they discuss how preconditions can be detected and used to detect merge conflicts.

\citet{Apel} describes the idea of semistructured merge and creates a framework that will be extensible to several languages. They parse programs into unordered trees in which the leaves are functions that contain unstructured program code. The code in leaves is then merged by language specific conflict handlers or by an unstructured merge. They found that merge conflicts are generally minimized, and conclude that many conflicts are ordering conflicts. In 26\% of the cases there will actually be more conflicts, due to renaming creating conflicts in this approach as opposed to compile errors in unstructured merge.

\citet{Olav} describes a system which first tries unstructured merge and afterwords does structured merge on conflicting JAVA files. The idea is to minimize the running time of the tools, to make them more convenient to use in real life scenarios. They report generally a lower amount of conflicts than unstructured merge and \citet{Apel} but in some scenarios, in the case of moved files, they produce more conflicts. They general cut off 50\% of the running time compared to a pure structured approach, are a factor 12 slower than a unstructured merge, and a factor 6 faster than \citet{Apel}. However the improvement factor varies a lot from repository to repository. 


\subsection{Tree differencing  and merging}
Research in the area has shown that the problem generally can be solved in cubic time. Several different algorithms exists, that all build on different dynamic programming decompositions. They perform well in different circumstances, and algorithms for cubic time have been created. \citet{Pawlik} describes and algorithm that will perform in cubic time, and that dynamically will decide if other algorithms would perform better for a specific sub-problem - this algorithm seems to be the most promising in regards to differencing and running time.

\citet{Zhang} describes the idea of constraining mapping, such that the one of the common ancestors of two nodes needs to be the same. This provides and algorithm that runs in quadratic time. Later \citet{Lu} described a loosened constraint and provided an algorithm that was quadratic, but also dependant on the degree of the tree.

Merging is not a time consuming problem, as long as an edit script is provided. The different existing merging algorithms that exists for three-way merging on trees generally describe different properties that is desirable for how to resolve conflicts, or when to produce them. \citet{lindholm} describes a merging algorithm for XML documents, \citet{Horwitz,Asklund,Olav} describes merging properties interesting for program code.

\todo{Ting der arbejder på tvaers af sprog:
Semistroctored Merge: Rethinking Merge in Revision Control Systems
Diff/TS: A tool for fine grained....
}
\clearpage

\section{Discussions}

\subsection{Cross-language interfaces}
Stuff \todo{write about this}

\subsection{Algorithm}
Several people already have looked into structural differencing and merging of structured data and specifically program code. This research seems however to focus mostly on the conflict resolution part, and glances over the problem of running times. While some algorithmic research has been done with regards to approximation algorithms and creating effective exact tree differencing algorithms, they are still prone running times many factors larger than unstructured approaches due to the tree size, counted in nodes, is generally much larger than the amount of lines in a code file. For example, running the \citet{Pawlik} algorithm on a 700 line C\# file \todo{Put this file in an appendix}, yields a syntax tree of 3800 nodes which gives running time for the matching of more than two minutes.

Every developer has their own preferences of development environment, which makes it essential that a tool can work with many environments, and as such a operation based approach where changes are logged by an editor, seems quite impractical, so we will keep the integration in the version control systems. Also, structured differencing tools will be only for specific languages or structures. This makes such tools impractical in version control systems, since they are designed to integrate with line based tools, and do not discriminate between file types. Some research has been done into making tools that will work with either several languages, or that combines a line based approach with a structured approach.
\todo[inline]{Write about assumptions in differencing and about different possible differencing operations}
\todo[inline]{Write about which situations we can probably avoid}


\clearpage

\section{Proposed solution}

\subsection{External interface}
\todo{write about this}


\subsection{Cross-language interfaces}
\todo{write about this}

\todo[inline]{Preprocessing directives}
It does not seem plausible to be able to make an algorithm that will make differencing an entire code file as a syntax tree possible in reasonable time.

I will implement a merging algorithm that is generally syntax based, but which also relies heavily on line merging.

First syntax trees are generated. These are parsed recursively until class members are encountered. Class members will be matched unordered from a heuristic cost function, and when this matchings is finished the body of the members will be merged textually. If this produces a conflict, we will instead try a tree-merge of the triple of functions.  Lastly we will have to reassemble the tree, in a ordering that is expected by the user. 

An overall description for the proposed merge:

\begin{enumerate}
	\item Create syntax trees of each file.
	\item Do an unordered match of classes and class-members.
	\item Foreach member of a class do a unstructured merge.
	\item If a conflict exists after the unstructured merge:
	\begin{enumerate}
		\item If the conflict is two updates on the same line, try to do a syntax tree merging. If the conflict persists, merge the entire function and include a rename detection.
		\item If the conflict is the deletion of a block in one branch, and the insertion of the other - put this specific line aside for move detection.
	\end{enumerate}
	\item Run through lines set aside for move detection, and see if it is possible to find a (heuristicly) corresponding insert. Do a merge, or a syntactic merge if this fails.
	\item If conflicts cannot be resolved, present these to the user.
\end{enumerate}

\subsection{Running time}
This approach will most definitely be slower than running a line based merging algorithm, however it will hopefully fast enough to be usable in a version control scenario.

Doing a textual merge on class-member will yield results that are comparable to current merging approaches, so the added running time of this algorithm will be from two factors: Doing a three-way cost matching on class members, and the added tree matching whenever a conflict is encountered.

Matching class members can be done in cubic time, on the amount of nodes. For each node pair we need to generate a cost, the running time of this function should be quite low, and we will discuss later how it turns out. 

Tree matching will still have a cubic running time on the amount of nodes, however with the assumptions that conflicts are concentrated around a few functions and that functions are generally small, the running time of this part should not influence the overall running time too much.

\subsection{Implementation details}
This leaves us with three questions that are open for discussions.

\subsubsection{Three-way cost matching}
\todo{Why do we use the reduction to flow algoirhtm, and not hungarian method?}
Matching the members $(l_1, l_2, ..., l_{size(l)})$, $(b_1, b_2, ..., b_{size(b)})$ and $(r_1, r_2, ..., r_{size(r)})$ will be done by making a bipartite matching between the pairs $(b, l)$ and $(b, r)$, and afterwards matching the matching on the base items.

Finding a perfect bipartite matching with minimum costs can be solved in cubic time by \citet{bipartitecost}. This works if one is given equal size sets, and given the assumption that a perfect matching exists.

There will be no guarantee that the different function sets are equal in size, or even that a similarity function will produce an edge between two nodes. This means there will be no guarantee for a possible perfect matching in the input. 

Therefore we will redefine the algorithm such that it only runs as long as a path exists between source and sink in the residual graph. All remaining items not in the matching will generate matching of either $(x, \epsilon)$ or $(\epsilon, y)$ in the output.

Given the left to base and right to base matching, we will do another unordered matching on the base-items.

\todo[inline]{We could probably just sort them after $b$-items, and then do an ordered matching.}

An over approximation of the running time here will be  $3*max(x, y, z)^3$.

\subsubsection{Function similarity measure}
Doing cost-based matching, means we also will have to define function similarity measures. This function will have to be run for each $(l, b)$ and $(r, b)$-pair, which means it is important to keep the running time low.

Four overall parameters are obvious as similarity measures:

\begin{enumerate}
	\item The identifier of a class member.
	\item The parameters of a class member.
	\item The modifiers of a class member.
	\item The body of a class member.
\end{enumerate}

For identifiers equal strings will of course have to be the lowest cost. Besides that, one could think up different string similarity measures that would allow different renames of a member to have lower costs than others.
\todo[inline]{Needs to elaborate alot}

\subsubsection{Ordering of outputted class members}
The end result of the matching algorithm will be a set of merged class members. 

Conflicts might exists if two users have moved the same function into different places in the left and right branches, but that is the only case where user intervention should be needed. However there are quite a few cases of reordering operations, where the answer of where to actual position a function in the output is simply undefined. 


\todo{Match default? Forrest or tree? Do we allways know that the root node matches?}

\clearpage
\section{Evaluation}

\clearpage
\section{Conclusion}

\clearpage

\bibliographystyle{plainnat}
\bibliography{libary}

\end{document}

